{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbb20b9",
   "metadata": {},
   "source": [
    "# Install & Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c15ec69-6b2b-4446-95fe-010faddde0f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71069a1b-5b8f-423c-8b18-1d783ca7658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from h5py) (1.24.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (3.3.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorflow) (1.24.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: rich in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.4.2)\n",
      "Requirement already satisfied: namex in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr4/ba820/wym1205/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f9d250-1b70-4186-8b59-8bf9d5f56809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 20:51:06.655575: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-25 20:51:06.700122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 20:51:08.388864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, LSTM, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.image import resize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0a904",
   "metadata": {},
   "source": [
    "# Load Dataset & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e12eac-7ba4-4e81-932c-555a99c196f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /projectnb/ba865/projects/Group3_A1\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6061b679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract MFCCs and labels from Pickle files\n",
    "base_path = '/projectnb/ba865/projects/Group3_A1/dataset_pkl'\n",
    "pkl_files = [file for file in os.listdir(base_path) if file.endswith('.pkl')]\n",
    "\n",
    "label_counts = {}\n",
    "all_mfccs = []\n",
    "all_labels = []\n",
    "\n",
    "max_cols = 0\n",
    "for pkl_file in pkl_files:\n",
    "    file_path = os.path.join(base_path, pkl_file)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        i = 0\n",
    "        for item in data:\n",
    "            if i >= 20000:\n",
    "                break\n",
    "            mfccs = item['mfccs']\n",
    "            labels = item['label']\n",
    "            \n",
    "            # this was previously used to count the number of MFCCs from each language and reduce sample size\n",
    "            if labels not in label_counts:\n",
    "                label_counts[labels] = 0\n",
    "            label_counts[labels] += 1\n",
    "\n",
    "            # tracks max_cols to pad correctly later\n",
    "            max_cols = max(max_cols, mfccs.shape[1])\n",
    "\n",
    "            all_mfccs.append(mfccs)\n",
    "            all_labels.append(labels)\n",
    "            i += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95e01140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'russian': 20000,\n",
       " 'thai': 20000,\n",
       " 'arabic': 20000,\n",
       " 'japanese': 20000,\n",
       " 'english': 20000,\n",
       " 'chinese': 20000,\n",
       " 'spanish': 20000,\n",
       " 'german': 20000,\n",
       " 'french': 20000}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d5d4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'russian'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b8e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the MFCCs tensor: (180000, 13, 972)\n",
      "Shape of the labels tensor: (180000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 20:51:23.328447: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# pad MFCCs to ensure same shape across\n",
    "padded_mfccs = [np.pad(mfcc, ((0, 0), (0, max_cols - mfcc.shape[1])), mode='constant', constant_values=0) for mfcc in all_mfccs]\n",
    "\n",
    "mfccs = np.stack(padded_mfccs)\n",
    "\n",
    "# encode the lebels\n",
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(all_labels)\n",
    "\n",
    "# convert MFCCs and labels to tensor\n",
    "mfccs_tensor=tf.convert_to_tensor(padded_mfccs,dtype=tf.float32)\n",
    "labels_tensor = tf.convert_to_tensor(np.array(labels_encoded), dtype=tf.int32)\n",
    "\n",
    "print(f\"Shape of the MFCCs tensor: {mfccs_tensor.shape}\")\n",
    "print(f\"Shape of the labels tensor: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c02cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10528b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoded.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "568d936b-bb8f-439b-93ce-1ff6ba87c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs, labels_encoded, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# reshape for model input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
    "\n",
    "# encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train_one_hot = to_categorical(y_train_encoded, num_classes=9)\n",
    "y_val_one_hot = to_categorical(y_val_encoded, num_classes=9)\n",
    "y_test_one_hot = to_categorical(y_test_encoded, num_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ad05b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115200, 13, 972, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7b898",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e4a0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest test accuracy: 0.4428888888888889\n"
     ]
    }
   ],
   "source": [
    "# flatten the data to feed into the model\n",
    "mfccs_array=np.array(mfccs_tensor)\n",
    "mfccs_flattened = np.array([mfcc.flatten() for mfcc in mfccs_array])\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs_flattened, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize and train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train) \n",
    "\n",
    "# evaluate the model and print accuracy\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print('Random Forest test accuracy:', accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c6c9d",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520b4f6d-267f-46c2-954b-5c877707d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 212ms/step - accuracy: 0.4016 - loss: 1.6902 - val_accuracy: 0.5042 - val_loss: 1.4501\n",
      "Epoch 2/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 211ms/step - accuracy: 0.5469 - loss: 1.3128 - val_accuracy: 0.5562 - val_loss: 1.2758\n",
      "Epoch 3/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 215ms/step - accuracy: 0.5681 - loss: 1.2456 - val_accuracy: 0.5815 - val_loss: 1.2370\n",
      "Epoch 4/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 213ms/step - accuracy: 0.5904 - loss: 1.1902 - val_accuracy: 0.5500 - val_loss: 1.2849\n",
      "Epoch 5/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 211ms/step - accuracy: 0.5970 - loss: 1.1685 - val_accuracy: 0.6031 - val_loss: 1.1597\n",
      "Epoch 6/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 211ms/step - accuracy: 0.6106 - loss: 1.1373 - val_accuracy: 0.5878 - val_loss: 1.1935\n",
      "Epoch 7/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 211ms/step - accuracy: 0.6179 - loss: 1.1190 - val_accuracy: 0.6214 - val_loss: 1.0956\n",
      "Epoch 8/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 213ms/step - accuracy: 0.6234 - loss: 1.0966 - val_accuracy: 0.6070 - val_loss: 1.1598\n",
      "Epoch 9/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 211ms/step - accuracy: 0.6281 - loss: 1.0838 - val_accuracy: 0.4677 - val_loss: 1.8230\n",
      "Epoch 10/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 210ms/step - accuracy: 0.6333 - loss: 1.0732 - val_accuracy: 0.5312 - val_loss: 1.4763\n",
      "Epoch 11/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 211ms/step - accuracy: 0.6346 - loss: 1.0638 - val_accuracy: 0.6187 - val_loss: 1.1302\n",
      "Epoch 12/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 210ms/step - accuracy: 0.6402 - loss: 1.0556 - val_accuracy: 0.6217 - val_loss: 1.1319\n",
      "Epoch 13/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 210ms/step - accuracy: 0.6419 - loss: 1.0451 - val_accuracy: 0.6318 - val_loss: 1.0821\n",
      "Epoch 14/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 211ms/step - accuracy: 0.6437 - loss: 1.0423 - val_accuracy: 0.6172 - val_loss: 1.1343\n",
      "Epoch 15/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 210ms/step - accuracy: 0.6481 - loss: 1.0260 - val_accuracy: 0.6469 - val_loss: 1.0335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1504c0f62d70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# design CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile and fit the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_one_hot, epochs=15, batch_size=64, validation_data=(X_val, y_val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95ae8a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - accuracy: 0.6545 - loss: 1.0259\n",
      "Test accuracy: 0.6516388654708862\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83de963",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afd12966-dc57-4a23-99b1-75176a948505",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.2784 - loss: 1.9457 - val_accuracy: 0.4018 - val_loss: 1.6881\n",
      "Epoch 2/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4008 - loss: 1.6916 - val_accuracy: 0.4333 - val_loss: 1.6129\n",
      "Epoch 3/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4402 - loss: 1.6016 - val_accuracy: 0.4527 - val_loss: 1.5622\n",
      "Epoch 4/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4605 - loss: 1.5499 - val_accuracy: 0.4607 - val_loss: 1.5378\n",
      "Epoch 5/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4717 - loss: 1.5065 - val_accuracy: 0.4712 - val_loss: 1.5084\n",
      "Epoch 6/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4871 - loss: 1.4779 - val_accuracy: 0.4842 - val_loss: 1.4837\n",
      "Epoch 7/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4976 - loss: 1.4472 - val_accuracy: 0.4927 - val_loss: 1.4580\n",
      "Epoch 8/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5062 - loss: 1.4262 - val_accuracy: 0.4962 - val_loss: 1.4529\n",
      "Epoch 9/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5122 - loss: 1.4069 - val_accuracy: 0.5049 - val_loss: 1.4301\n",
      "Epoch 10/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5222 - loss: 1.3827 - val_accuracy: 0.5100 - val_loss: 1.4131\n",
      "Epoch 11/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5282 - loss: 1.3634 - val_accuracy: 0.5126 - val_loss: 1.4059\n",
      "Epoch 12/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5329 - loss: 1.3490 - val_accuracy: 0.5172 - val_loss: 1.3988\n",
      "Epoch 13/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5357 - loss: 1.3366 - val_accuracy: 0.5182 - val_loss: 1.3873\n",
      "Epoch 14/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5425 - loss: 1.3248 - val_accuracy: 0.5192 - val_loss: 1.3972\n",
      "Epoch 15/15\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.5486 - loss: 1.3050 - val_accuracy: 0.5238 - val_loss: 1.3784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1504641bd840>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# design LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    Dropout(0.1),  # Apply dropout after the first LSTM layer\n",
    "    BatchNormalization(),  # Optionally add batch normalization (though not typical for LSTMs)\n",
    "    \n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.1),  # Apply dropout after the second LSTM layer\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),  # Additional dropout before the final layer\n",
    "    \n",
    "    Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile and fit the model\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train_one_hot, epochs=15, batch_size=64, validation_data=(X_val, y_val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d62a8fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.5252 - loss: 1.3805\n",
      "Test accuracy: 0.5286666750907898\n"
     ]
    }
   ],
   "source": [
    "# reshape X_test\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "# evaluate model accuracy on test set\n",
    "test_loss, test_acc = lstm_model.evaluate(X_test, y_test_one_hot)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5409c6d",
   "metadata": {},
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6ab6ce2-ce2e-4a82-a27d-9652fba3f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 53ms/step - accuracy: 0.2636 - loss: 1.9438 - val_accuracy: 0.3776 - val_loss: 1.7404\n",
      "Epoch 2/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 47ms/step - accuracy: 0.3975 - loss: 1.6829 - val_accuracy: 0.4218 - val_loss: 1.6307\n",
      "Epoch 3/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 49ms/step - accuracy: 0.4376 - loss: 1.5966 - val_accuracy: 0.4784 - val_loss: 1.4886\n",
      "Epoch 4/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 49ms/step - accuracy: 0.4651 - loss: 1.5396 - val_accuracy: 0.4805 - val_loss: 1.4739\n",
      "Epoch 5/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 50ms/step - accuracy: 0.4695 - loss: 1.5257 - val_accuracy: 0.4595 - val_loss: 1.5402\n",
      "Epoch 6/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 50ms/step - accuracy: 0.4816 - loss: 1.4974 - val_accuracy: 0.5185 - val_loss: 1.4073\n",
      "Epoch 7/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 51ms/step - accuracy: 0.4927 - loss: 1.4780 - val_accuracy: 0.5075 - val_loss: 1.4137\n",
      "Epoch 8/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 51ms/step - accuracy: 0.4973 - loss: 1.4693 - val_accuracy: 0.5219 - val_loss: 1.3826\n",
      "Epoch 9/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 51ms/step - accuracy: 0.5018 - loss: 1.4552 - val_accuracy: 0.4792 - val_loss: 1.4783\n",
      "Epoch 10/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 49ms/step - accuracy: 0.5070 - loss: 1.4398 - val_accuracy: 0.5370 - val_loss: 1.3468\n",
      "Epoch 11/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 47ms/step - accuracy: 0.5125 - loss: 1.4207 - val_accuracy: 0.5269 - val_loss: 1.3747\n",
      "Epoch 12/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 47ms/step - accuracy: 0.5149 - loss: 1.4252 - val_accuracy: 0.5234 - val_loss: 1.3783\n",
      "Epoch 13/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 47ms/step - accuracy: 0.5187 - loss: 1.4155 - val_accuracy: 0.5291 - val_loss: 1.3689\n",
      "Epoch 14/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 47ms/step - accuracy: 0.5216 - loss: 1.4094 - val_accuracy: 0.5263 - val_loss: 1.3769\n",
      "Epoch 15/15\n",
      "\u001b[1m3600/3600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 48ms/step - accuracy: 0.5285 - loss: 1.3918 - val_accuracy: 0.5326 - val_loss: 1.3734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x150e7c66edd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# design CRNN model\n",
    "crnn_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    TimeDistributed(Flatten()),\n",
    "    \n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.1),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile and fit the model\n",
    "crnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "crnn_model.fit(X_train, y_train_one_hot, epochs=15, batch_size=32, validation_data=(X_val, y_val_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f9e81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.5350 - loss: 1.3629\n",
      "Test accuracy: 0.5353333353996277\n",
      "Test loss: 1.3585960865020752\n"
     ]
    }
   ],
   "source": [
    "# reshape X_test and convert X_test and y_test_one_hot to tensor\n",
    "X_test = X_test[..., np.newaxis] \n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test_one_hot = tf.convert_to_tensor(y_test_one_hot, dtype=tf.float32)\n",
    "\n",
    "# evaluate model accuracy on test set\n",
    "test_loss, test_acc = crnn_model.evaluate(X_test, y_test_one_hot)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d08256",
   "metadata": {},
   "source": [
    "# Transfer Learning: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77019113",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.2263 - loss: 2.1173 - val_accuracy: 0.2647 - val_loss: 1.9887\n",
      "Epoch 2/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 24ms/step - accuracy: 0.2691 - loss: 1.9838 - val_accuracy: 0.2734 - val_loss: 1.9601\n",
      "Epoch 3/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.2769 - loss: 1.9601 - val_accuracy: 0.2789 - val_loss: 1.9472\n",
      "Epoch 4/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 24ms/step - accuracy: 0.2854 - loss: 1.9429 - val_accuracy: 0.2830 - val_loss: 1.9485\n",
      "Epoch 5/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 26ms/step - accuracy: 0.2912 - loss: 1.9322 - val_accuracy: 0.2920 - val_loss: 1.9281\n",
      "Epoch 6/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.2897 - loss: 1.9288 - val_accuracy: 0.2873 - val_loss: 1.9334\n",
      "Epoch 7/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.3004 - loss: 1.9113 - val_accuracy: 0.2916 - val_loss: 1.9254\n",
      "Epoch 8/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.3017 - loss: 1.9086 - val_accuracy: 0.2967 - val_loss: 1.9180\n",
      "Epoch 9/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.3029 - loss: 1.9071 - val_accuracy: 0.3006 - val_loss: 1.9192\n",
      "Epoch 10/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 24ms/step - accuracy: 0.3047 - loss: 1.9007 - val_accuracy: 0.2928 - val_loss: 1.9199\n",
      "Epoch 11/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.3048 - loss: 1.9016 - val_accuracy: 0.3046 - val_loss: 1.9123\n",
      "Epoch 12/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 24ms/step - accuracy: 0.3047 - loss: 1.9005 - val_accuracy: 0.3053 - val_loss: 1.9110\n",
      "Epoch 13/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 26ms/step - accuracy: 0.3093 - loss: 1.8911 - val_accuracy: 0.3037 - val_loss: 1.9172\n",
      "Epoch 14/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 24ms/step - accuracy: 0.3088 - loss: 1.8923 - val_accuracy: 0.2997 - val_loss: 1.9180\n",
      "Epoch 15/15\n",
      "\u001b[1m2880/2880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 24ms/step - accuracy: 0.3116 - loss: 1.8904 - val_accuracy: 0.2993 - val_loss: 1.9151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15115e08e770>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert X_train to match the model\n",
    "X_train_rgb = np.repeat(X_train, 3, axis=-1)\n",
    "\n",
    "# resize X_train\n",
    "X_train_resized = np.array([resize(img, (32, 32)).numpy() for img in X_train_rgb])\n",
    "\n",
    "# encode y_train\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_train_one_hot = to_categorical(y_train_encoded, num_classes=9)\n",
    "\n",
    "# train test split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_resized, y_train_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# load VGG16 model without the final layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add custom layers\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(9, activation='softmax')(x)\n",
    "\n",
    "# create the model\n",
    "transfer_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "transfer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "transfer_model.fit(X_train_split, y_train_split, epochs=15, batch_size=32, validation_data=(X_val_split, y_val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b2e4838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 21ms/step - accuracy: 0.3025 - loss: 1.9162\n",
      "Test accuracy: 0.300861120223999\n"
     ]
    }
   ],
   "source": [
    "# convert X_test to match the model\n",
    "X_test_rgb = np.repeat(X_test[..., np.newaxis], 3, axis=-1)  \n",
    "\n",
    "# resize X_test\n",
    "X_test_resized = np.array([resize(img, (32, 32)).numpy() for img in X_test_rgb])\n",
    "\n",
    "# encode y_test\n",
    "y_test_encoded = label_encoder.transform(y_test)  # using the same encoder as for y_train\n",
    "y_test_one_hot = to_categorical(y_test_encoded, num_classes=9)\n",
    "\n",
    "# evaluate model's accuracy on test set\n",
    "test_loss, test_acc = transfer_model.evaluate(X_test_resized, y_test_one_hot)\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
